{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assigment, we will work with the *Forest Fire* data set. Please download the data from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/dataset/162/forest+fires). Extract the data files into the subdirectory: `../data/fires/` (relative to `./src/`).\n",
    "\n",
    "## Objective\n",
    "\n",
    "+ The model objective is to predict the area affected by forest fires given the features set. \n",
    "+ The objective of this exercise is to assess your ability to construct and evaluate model pipelines.\n",
    "+ Please note: the instructions are not meant to be 100% prescriptive, but instead they are a set of minimum requirements. If you find predictive performance gains by applying additional steps, by all means show them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Description\n",
    "\n",
    "From the description file contained in the archive (`forestfires.names`), we obtain the following variable descriptions:\n",
    "\n",
    "1. X - x-axis spatial coordinate within the Montesinho park map: 1 to 9\n",
    "2. Y - y-axis spatial coordinate within the Montesinho park map: 2 to 9\n",
    "3. month - month of the year: \"jan\" to \"dec\" \n",
    "4. day - day of the week: \"mon\" to \"sun\"\n",
    "5. FFMC - FFMC index from the FWI system: 18.7 to 96.20\n",
    "6. DMC - DMC index from the FWI system: 1.1 to 291.3 \n",
    "7. DC - DC index from the FWI system: 7.9 to 860.6 \n",
    "8. ISI - ISI index from the FWI system: 0.0 to 56.10\n",
    "9. temp - temperature in Celsius degrees: 2.2 to 33.30\n",
    "10. RH - relative humidity in %: 15.0 to 100\n",
    "11. wind - wind speed in km/h: 0.40 to 9.40 \n",
    "12. rain - outside rain in mm/m2 : 0.0 to 6.4 \n",
    "13. area - the burned area of the forest (in ha): 0.00 to 1090.84 \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "### Specific Tasks\n",
    "\n",
    "+ Construct four model pipelines, out of combinations of the following components:\n",
    "\n",
    "    + Preprocessors:\n",
    "\n",
    "        - A simple processor that only scales numeric variables and recodes categorical variables.\n",
    "        - A transformation preprocessor that scales numeric variables and applies a non-linear transformation.\n",
    "    \n",
    "    + Regressor:\n",
    "\n",
    "        - A baseline regressor, which could be a [K-nearest neighbours model]() or a linear model like [Lasso](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html) or [Ridge Regressors](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ridge_regression.html).\n",
    "        - An advanced regressor of your choice (e.g., Bagging, Boosting, SVR, etc.). TIP: select a tree-based method such that it does not take too long to run SHAP further below. \n",
    "\n",
    "+ Evaluate tune and evaluate each of the four model pipelines. \n",
    "\n",
    "    - Select a [performance metric](https://scikit-learn.org/stable/modules/linear_model.html) out of the following options: explained variance, max error, root mean squared error (RMSE), mean absolute error (MAE), r-squared.\n",
    "    - *TIPS*: \n",
    "    \n",
    "        * Out of the suggested metrics above, [some are correlation metrics, but this is a prediction problem](https://www.tmwr.org/performance#performance). Choose wisely (and don't choose the incorrect options.) \n",
    "\n",
    "+ Select the best-performing model and explain its predictions.\n",
    "\n",
    "    - Provide local explanations.\n",
    "    - Obtain global explanations and recommend a variable selection strategy.\n",
    "\n",
    "+ Export your model as a pickle file.\n",
    "\n",
    "\n",
    "You can work on the Jupyter notebook, as this experiment is fairly short (no need to use sacred). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data\n",
    "\n",
    "Place the files in the ../../05_src/data/fires/ directory and load the appropriate file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/vishalbansur/production/02_activities/assignments\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('../../05_src/data/forestfires.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 517 entries, 0 to 516\n",
      "Data columns (total 13 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   coord_x  517 non-null    int64  \n",
      " 1   coord_y  517 non-null    int64  \n",
      " 2   month    517 non-null    object \n",
      " 3   day      517 non-null    object \n",
      " 4   ffmc     517 non-null    float64\n",
      " 5   dmc      517 non-null    float64\n",
      " 6   dc       517 non-null    float64\n",
      " 7   isi      517 non-null    float64\n",
      " 8   temp     517 non-null    float64\n",
      " 9   rh       517 non-null    int64  \n",
      " 10  wind     517 non-null    float64\n",
      " 11  rain     517 non-null    float64\n",
      " 12  area     517 non-null    float64\n",
      "dtypes: float64(8), int64(3), object(2)\n",
      "memory usage: 52.6+ KB\n"
     ]
    }
   ],
   "source": [
    "# Load the data with default headers, then rename columns\n",
    "fires_dt = pd.read_csv('../../05_src/data/forestfires.csv')\n",
    "\n",
    "# Rename the columns after loading\n",
    "fires_dt.columns = [\n",
    "    'coord_x', 'coord_y', 'month', 'day', 'ffmc', 'dmc', 'dc', 'isi', 'temp', 'rh', 'wind', 'rain', 'area' \n",
    "]\n",
    "\n",
    "# Verify the data\n",
    "fires_dt.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get X and Y\n",
    "\n",
    "Create the features data frame and target data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../../05_src/data/forestfires.csv')\n",
    "\n",
    "# X (features) will include all columns except 'area' (which is the target)\n",
    "X = df.drop('area', axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y (target) will be the 'area' column\n",
    "Y = df['area']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "Create two [Column Transformers](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html), called preproc1 and preproc2, with the following guidelines:\n",
    "\n",
    "- Numerical variables\n",
    "\n",
    "    * (Preproc 1 and 2) Scaling: use a scaling method of your choice (Standard, Robust, Min-Max). \n",
    "    * Preproc 2 only: \n",
    "        \n",
    "        + Choose a transformation for any of your input variables (or several of them). Evaluate if this transformation is convenient.\n",
    "        + The choice of scaler is up to you.\n",
    "\n",
    "- Categorical variables: \n",
    "    \n",
    "    * (Preproc 1 and 2) Apply [one-hot encoding](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) where appropriate.\n",
    "\n",
    "\n",
    "+ The only difference between preproc1 and preproc2 is the non-linear transformation of the numerical variables.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preproc 1\n",
    "\n",
    "Create preproc1 below.\n",
    "\n",
    "+ Numeric: scaled variables, no other transforms.\n",
    "+ Categorical: one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PowerTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "numeric_cols = X.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Preprocessing for preproc1: scaling and one-hot encoding\n",
    "preproc1 = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_cols),   # Scaling the numerical columns using StandardScaler\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)  # One-hot encoding categorical columns\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preproc 2\n",
    "\n",
    "Create preproc1 below.\n",
    "\n",
    "+ Numeric: scaled variables, non-linear transformation to one or more variables.\n",
    "+ Categorical: one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing for preproc2: scaling, non-linear transformation, and one-hot encoding\n",
    "preproc2 = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('scaler', StandardScaler()),             # Standard scaling\n",
    "            ('power_transform', PowerTransformer())    # Power transformation (non-linear)\n",
    "        ]), numeric_cols),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)  # One-hot encoding categorical columns\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Fit and transform the data using preproc1 and preproc2\n",
    "X_preproc1 = preproc1.fit_transform(X)\n",
    "X_preproc2 = preproc2.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Pipeline\n",
    "\n",
    "\n",
    "Create a [model pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html): \n",
    "\n",
    "+ Add a step labelled `preprocessing` and assign the Column Transformer from the previous section.\n",
    "+ Add a step labelled `regressor` and assign a regression model to it. \n",
    "\n",
    "## Regressor\n",
    "\n",
    "+ Use a regression model to perform a prediction. \n",
    "\n",
    "    - Choose a baseline regressor, tune it (if necessary) using grid search, and evaluate it using cross-validation.\n",
    "    - Choose a more advance regressor, tune it (if necessary) using grid search, and evaluate it using cross-validation.\n",
    "    - Both model choices are up to you, feel free to experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Example: Assuming you have numeric and categorical features\n",
    "numeric_features = ['age', 'income', 'years_of_experience']  # List your numeric columns\n",
    "categorical_features = ['gender', 'education_level']  # List your categorical columns\n",
    "\n",
    "# Preprocessing for numeric data: imputation + scaling\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),  # Fill missing values with mean\n",
    "    ('scaler', StandardScaler())  # Standardize numeric features\n",
    "])\n",
    "\n",
    "# Preprocessing for categorical data: imputation + one-hot encoding\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Fill missing values with the most frequent category\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))  # One-hot encode categorical features\n",
    "])\n",
    "\n",
    "# Create the column transformer to apply the right preprocessing to the appropriate columns\n",
    "preprocessor1 = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Baseline model: Linear Regression\n",
    "baseline_regressor = LinearRegression()\n",
    "\n",
    "# Advanced model: RandomForestRegressor (or you can choose another)\n",
    "advanced_regressor = RandomForestRegressor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline A = preproc1 + baseline\n",
    "# Pipeline A: Preprocessing1 + Baseline\n",
    "pipeline_a = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor1),  # Preprocessing step\n",
    "    ('regressor', baseline_regressor)  # Baseline regressor (Linear Regression)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline B = preproc2 + baseline\n",
    "# Pipeline B: Preprocessing2 + Baseline\n",
    "# Here we assume a different preprocessing pipeline (e.g., different scaling/encoding)\n",
    "# You can define preprocessor2 similarly as preprocessor1\n",
    "# pipeline_b = Pipeline(steps=[\n",
    "#    ('preprocessor', preprocessor2),  # Preprocessing step 2\n",
    "#    ('regressor', baseline_regressor)  # Baseline regressor (Linear Regression)\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline C = preproc1 + advanced model\n",
    "# Pipeline C: Preprocessing1 + Advanced Model (RandomForest)\n",
    "pipeline_c = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor1),  # Preprocessing step\n",
    "    ('regressor', advanced_regressor)  # Advanced regressor (RandomForest)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline D = preproc2 + advanced model\n",
    "\n",
    "    # Pipeline D: Preprocessing2 + Advanced Model (RandomForest)\n",
    "# pipeline_d = Pipeline(steps=[\n",
    "#    ('preprocessor', preprocessor2),  # Preprocessing step 2\n",
    "#    ('regressor', advanced_regressor)  # Advanced regressor (RandomForest)\n",
    "# ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune Hyperparams\n",
    "\n",
    "+ Perform GridSearch on each of the four pipelines. \n",
    "+ Tune at least one hyperparameter per pipeline.\n",
    "+ Experiment with at least four value combinations per pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   X  Y month  day  FFMC   DMC     DC  ISI  temp  RH  wind  rain\n",
      "0  7  5   mar  fri  86.2  26.2   94.3  5.1   8.2  51   6.7   0.0\n",
      "1  7  4   oct  tue  90.6  35.4  669.1  6.7  18.0  33   0.9   0.0\n",
      "2  7  4   oct  sat  90.6  43.7  686.9  6.7  14.6  33   1.3   0.0\n",
      "3  8  6   mar  fri  91.7  33.3   77.5  9.0   8.3  97   4.0   0.2\n",
      "4  8  6   mar  sun  89.3  51.3  102.2  9.6  11.4  99   1.8   0.0\n",
      "0    0.0\n",
      "1    0.0\n",
      "2    0.0\n",
      "3    0.0\n",
      "4    0.0\n",
      "Name: area, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('/Users/vishalbansur/production/02_activities/assignments/forestfires.csv')\n",
    "\n",
    "# Define the feature matrix X and target variable y\n",
    "X = df.drop('area', axis=1)  # Dropping 'area' to get features\n",
    "y = df['area']  # Target variable\n",
    "\n",
    "# Check that the setup was successful\n",
    "print(X.head())  # Inspect the feature matrix\n",
    "print(y.head())  # Inspect the target variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df = pd.read_csv('/Users/vishalbansur/production/02_activities/assignments/forestfires.csv')\n",
    "\n",
    "# Define X and y, with `area` as the target variable\n",
    "X = df.drop('area', axis=1)\n",
    "y = df['area']\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Use RandomForestRegressor instead of RandomForestClassifier\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', RandomForestRegressor())\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'regressor__n_estimators': [100, 200],\n",
    "    'regressor__max_depth': [10, 20]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = ['month', 'day']\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Define the categorical columns\n",
    "categorical_columns = ['month', 'day']  # Replace with your actual categorical columns\n",
    "\n",
    "# Define the column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_columns)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Now you can apply this preprocessor to your dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_columns)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the numerical and categorical features\n",
    "numerical_features = ['FFMC', 'DMC', 'DC', 'ISI', 'temp', 'RH', 'wind', 'rain']\n",
    "categorical_features = ['month', 'day']\n",
    "\n",
    "# Create the column transformer to preprocess the features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(), categorical_features)\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the pipeline with RandomForestRegressor\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', RandomForestRegressor())\n",
    "])\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'regressor__n_estimators': [100, 200],   # Number of trees in the forest\n",
    "    'regressor__max_depth': [10, 20]          # Max depth of each tree\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearch for linreg...\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "[CV] END .........................linreg__fit_intercept=True; total time=   0.0s\n",
      "[CV] END .........................linreg__fit_intercept=True; total time=   0.0s\n",
      "[CV] END ........................linreg__fit_intercept=False; total time=   0.0s\n",
      "[CV] END ........................linreg__fit_intercept=False; total time=   0.0s\n",
      "[CV] END .........................linreg__fit_intercept=True; total time=   0.0s\n",
      "[CV] END ........................linreg__fit_intercept=False; total time=   0.1s\n",
      "[CV] END .........................linreg__fit_intercept=True; total time=   0.1s\n",
      "[CV] END .........................linreg__fit_intercept=True; total time=   0.1s\n",
      "[CV] END ........................linreg__fit_intercept=False; total time=   0.1s\n",
      "[CV] END ........................linreg__fit_intercept=False; total time=   0.0s\n",
      "Best parameters for linreg: {'linreg__fit_intercept': False}\n",
      "Best score for linreg: -0.2921323457007617\n",
      "Running GridSearch for dtree...\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "[CV] END .....dtree__max_depth=3, dtree__min_samples_split=2; total time=   0.0s\n",
      "[CV] END .....dtree__max_depth=3, dtree__min_samples_split=2; total time=   0.0s\n",
      "[CV] END .....dtree__max_depth=3, dtree__min_samples_split=2; total time=   0.0s\n",
      "[CV] END .....dtree__max_depth=3, dtree__min_samples_split=2; total time=   0.0s\n",
      "[CV] END .....dtree__max_depth=3, dtree__min_samples_split=5; total time=   0.0s\n",
      "[CV] END .....dtree__max_depth=3, dtree__min_samples_split=2; total time=   0.0s\n",
      "[CV] END .....dtree__max_depth=3, dtree__min_samples_split=5; total time=   0.0s\n",
      "[CV] END .....dtree__max_depth=3, dtree__min_samples_split=5; total time=   0.0s\n",
      "[CV] END ....dtree__max_depth=3, dtree__min_samples_split=10; total time=   0.0s\n",
      "[CV] END ....dtree__max_depth=3, dtree__min_samples_split=10; total time=   0.0s\n",
      "[CV] END ....dtree__max_depth=3, dtree__min_samples_split=10; total time=   0.0s\n",
      "[CV] END ....dtree__max_depth=3, dtree__min_samples_split=10; total time=   0.0s\n",
      "[CV] END .....dtree__max_depth=3, dtree__min_samples_split=5; total time=   0.0s\n",
      "[CV] END ....dtree__max_depth=3, dtree__min_samples_split=20; total time=   0.0s\n",
      "[CV] END .....dtree__max_depth=3, dtree__min_samples_split=5; total time=   0.0s\n",
      "[CV] END ....dtree__max_depth=3, dtree__min_samples_split=20; total time=   0.0s\n",
      "[CV] END ....dtree__max_depth=3, dtree__min_samples_split=20; total time=   0.0s\n",
      "[CV] END .....dtree__max_depth=5, dtree__min_samples_split=2; total time=   0.0s\n",
      "[CV] END ....dtree__max_depth=3, dtree__min_samples_split=10; total time=   0.0s\n",
      "[CV] END .....dtree__max_depth=5, dtree__min_samples_split=2; total time=   0.0s\n",
      "[CV] END .....dtree__max_depth=5, dtree__min_samples_split=2; total time=   0.0s\n",
      "[CV] END .....dtree__max_depth=5, dtree__min_samples_split=5; total time=   0.0s\n",
      "[CV] END ....dtree__max_depth=3, dtree__min_samples_split=20; total time=   0.0s\n",
      "[CV] END ....dtree__max_depth=5, dtree__min_samples_split=10; total time=   0.0s\n",
      "[CV] END .....dtree__max_depth=5, dtree__min_samples_split=5; total time=   0.1s\n",
      "[CV] END ....dtree__max_depth=5, dtree__min_samples_split=10; total time=   0.0s\n",
      "[CV] END .....dtree__max_depth=5, dtree__min_samples_split=2; total time=   0.1s\n",
      "[CV] END .....dtree__max_depth=5, dtree__min_samples_split=2; total time=   0.1s\n",
      "[CV] END ....dtree__max_depth=5, dtree__min_samples_split=10; total time=   0.1s\n",
      "[CV] END ....dtree__max_depth=5, dtree__min_samples_split=20; total time=   0.0s\n",
      "[CV] END ....dtree__max_depth=5, dtree__min_samples_split=10; total time=   0.0s\n",
      "[CV] END .....dtree__max_depth=5, dtree__min_samples_split=5; total time=   0.0s\n",
      "[CV] END .....dtree__max_depth=5, dtree__min_samples_split=5; total time=   0.1s\n",
      "[CV] END ....dtree__max_depth=3, dtree__min_samples_split=20; total time=   0.1s\n",
      "[CV] END .....dtree__max_depth=5, dtree__min_samples_split=5; total time=   0.0s\n",
      "[CV] END ....dtree__max_depth=5, dtree__min_samples_split=10; total time=   0.0s\n",
      "[CV] END ....dtree__max_depth=5, dtree__min_samples_split=20; total time=   0.0s\n",
      "[CV] END ....dtree__max_depth=5, dtree__min_samples_split=20; total time=   0.0s\n",
      "[CV] END .....dtree__max_depth=7, dtree__min_samples_split=5; total time=   0.0s\n",
      "[CV] END ....dtree__max_depth=5, dtree__min_samples_split=20; total time=   0.0s\n",
      "[CV] END .....dtree__max_depth=7, dtree__min_samples_split=2; total time=   0.0s\n",
      "[CV] END ....dtree__max_depth=5, dtree__min_samples_split=20; total time=   0.0s\n",
      "[CV] END ....dtree__max_depth=7, dtree__min_samples_split=20; total time=   0.0s\n",
      "[CV] END .....dtree__max_depth=7, dtree__min_samples_split=2; total time=   0.1s\n",
      "[CV] END .....dtree__max_depth=7, dtree__min_samples_split=2; total time=   0.1s\n",
      "[CV] END .....dtree__max_depth=7, dtree__min_samples_split=5; total time=   0.0s\n",
      "[CV] END ....dtree__max_depth=7, dtree__min_samples_split=10; total time=   0.1s\n",
      "[CV] END .....dtree__max_depth=7, dtree__min_samples_split=5; total time=   0.1s\n",
      "[CV] END ....dtree__max_depth=7, dtree__min_samples_split=20; total time=   0.0s\n",
      "[CV] END .....dtree__max_depth=7, dtree__min_samples_split=2; total time=   0.1s[CV] END ....dtree__max_depth=10, dtree__min_samples_split=2; total time=   0.1s\n",
      "[CV] END .....dtree__max_depth=7, dtree__min_samples_split=2; total time=   0.0s\n",
      "[CV] END .....dtree__max_depth=7, dtree__min_samples_split=5; total time=   0.0s\n",
      "\n",
      "[CV] END .....dtree__max_depth=7, dtree__min_samples_split=5; total time=   0.0s\n",
      "[CV] END ....dtree__max_depth=7, dtree__min_samples_split=10; total time=   0.0s\n",
      "[CV] END ....dtree__max_depth=10, dtree__min_samples_split=2; total time=   0.0s\n",
      "[CV] END ...dtree__max_depth=10, dtree__min_samples_split=10; total time=   0.0s\n",
      "[CV] END ....dtree__max_depth=10, dtree__min_samples_split=2; total time=   0.0s\n",
      "[CV] END ...dtree__max_depth=10, dtree__min_samples_split=20; total time=   0.0s\n",
      "[CV] END ....dtree__max_depth=7, dtree__min_samples_split=10; total time=   0.0s\n",
      "[CV] END ....dtree__max_depth=7, dtree__min_samples_split=20; total time=   0.0s\n",
      "[CV] END ....dtree__max_depth=10, dtree__min_samples_split=5; total time=   0.0s\n",
      "[CV] END ....dtree__max_depth=10, dtree__min_samples_split=5; total time=   0.0s\n",
      "[CV] END ....dtree__max_depth=7, dtree__min_samples_split=20; total time=   0.0s\n",
      "[CV] END ....dtree__max_depth=7, dtree__min_samples_split=10; total time=   0.0s\n",
      "[CV] END ...dtree__max_depth=10, dtree__min_samples_split=10; total time=   0.0s\n",
      "[CV] END ....dtree__max_depth=10, dtree__min_samples_split=5; total time=   0.0s\n",
      "[CV] END ....dtree__max_depth=7, dtree__min_samples_split=10; total time=   0.0s\n",
      "[CV] END ....dtree__max_depth=10, dtree__min_samples_split=2; total time=   0.0s\n",
      "[CV] END ...dtree__max_depth=10, dtree__min_samples_split=20; total time=   0.0s\n",
      "[CV] END ....dtree__max_depth=10, dtree__min_samples_split=5; total time=   0.0s\n",
      "[CV] END ....dtree__max_depth=7, dtree__min_samples_split=20; total time=   0.0s\n",
      "[CV] END ...dtree__max_depth=10, dtree__min_samples_split=10; total time=   0.0s\n",
      "[CV] END ....dtree__max_depth=10, dtree__min_samples_split=2; total time=   0.0s\n",
      "[CV] END ...dtree__max_depth=10, dtree__min_samples_split=10; total time=   0.0s\n",
      "[CV] END ...dtree__max_depth=10, dtree__min_samples_split=20; total time=   0.0s\n",
      "[CV] END ....dtree__max_depth=10, dtree__min_samples_split=5; total time=   0.0s\n",
      "[CV] END ...dtree__max_depth=10, dtree__min_samples_split=20; total time=   0.0s\n",
      "[CV] END ...dtree__max_depth=10, dtree__min_samples_split=20; total time=   0.0s\n",
      "[CV] END ...dtree__max_depth=10, dtree__min_samples_split=10; total time=   0.0s\n",
      "Best parameters for dtree: {'dtree__max_depth': 3, 'dtree__min_samples_split': 5}\n",
      "Best score for dtree: -1.6478350412620535\n",
      "Running GridSearch for rf...\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "[CV] END ...............rf__max_depth=3, rf__n_estimators=50; total time=   0.1s\n",
      "[CV] END ...............rf__max_depth=3, rf__n_estimators=50; total time=   0.1s\n",
      "[CV] END ...............rf__max_depth=3, rf__n_estimators=50; total time=   0.1s\n",
      "[CV] END ...............rf__max_depth=3, rf__n_estimators=50; total time=   0.1s\n",
      "[CV] END ...............rf__max_depth=3, rf__n_estimators=50; total time=   0.2s\n",
      "[CV] END ..............rf__max_depth=3, rf__n_estimators=100; total time=   0.2s\n",
      "[CV] END ..............rf__max_depth=3, rf__n_estimators=100; total time=   0.2s\n",
      "[CV] END ..............rf__max_depth=3, rf__n_estimators=100; total time=   0.2s\n",
      "[CV] END ..............rf__max_depth=3, rf__n_estimators=100; total time=   0.3s\n",
      "[CV] END ..............rf__max_depth=3, rf__n_estimators=100; total time=   0.3s\n",
      "[CV] END ..............rf__max_depth=3, rf__n_estimators=200; total time=   0.5s\n",
      "[CV] END ..............rf__max_depth=3, rf__n_estimators=200; total time=   0.5s\n",
      "[CV] END ..............rf__max_depth=3, rf__n_estimators=200; total time=   0.6s\n",
      "[CV] END ..............rf__max_depth=3, rf__n_estimators=200; total time=   0.5s\n",
      "[CV] END ..............rf__max_depth=3, rf__n_estimators=200; total time=   0.5s\n",
      "[CV] END ...............rf__max_depth=5, rf__n_estimators=50; total time=   0.2s\n",
      "[CV] END ...............rf__max_depth=5, rf__n_estimators=50; total time=   0.2s\n",
      "[CV] END ...............rf__max_depth=5, rf__n_estimators=50; total time=   0.2s\n",
      "[CV] END ...............rf__max_depth=5, rf__n_estimators=50; total time=   0.2s\n",
      "[CV] END ...............rf__max_depth=5, rf__n_estimators=50; total time=   0.2s\n",
      "[CV] END ..............rf__max_depth=3, rf__n_estimators=300; total time=   0.8s\n",
      "[CV] END ..............rf__max_depth=5, rf__n_estimators=100; total time=   0.3s\n",
      "[CV] END ..............rf__max_depth=5, rf__n_estimators=100; total time=   0.3s\n",
      "[CV] END ..............rf__max_depth=3, rf__n_estimators=300; total time=   0.7s\n",
      "[CV] END ..............rf__max_depth=3, rf__n_estimators=300; total time=   0.7s\n",
      "[CV] END ..............rf__max_depth=5, rf__n_estimators=100; total time=   0.3s\n",
      "[CV] END ..............rf__max_depth=5, rf__n_estimators=100; total time=   0.3s\n",
      "[CV] END ..............rf__max_depth=5, rf__n_estimators=100; total time=   0.3s\n",
      "[CV] END ..............rf__max_depth=5, rf__n_estimators=200; total time=   0.5s\n",
      "[CV] END ..............rf__max_depth=5, rf__n_estimators=200; total time=   0.5s\n",
      "[CV] END ..............rf__max_depth=10, rf__n_estimators=50; total time=   0.2s\n",
      "[CV] END ..............rf__max_depth=5, rf__n_estimators=200; total time=   0.5s\n",
      "[CV] END ..............rf__max_depth=10, rf__n_estimators=50; total time=   0.2s\n",
      "[CV] END ..............rf__max_depth=3, rf__n_estimators=300; total time=   0.7s\n",
      "[CV] END ..............rf__max_depth=3, rf__n_estimators=300; total time=   0.6s\n",
      "[CV] END ..............rf__max_depth=10, rf__n_estimators=50; total time=   0.2s\n",
      "[CV] END ..............rf__max_depth=10, rf__n_estimators=50; total time=   0.2s\n",
      "[CV] END ..............rf__max_depth=5, rf__n_estimators=300; total time=   0.8s\n",
      "[CV] END ..............rf__max_depth=5, rf__n_estimators=200; total time=   0.5s\n",
      "[CV] END ..............rf__max_depth=5, rf__n_estimators=200; total time=   0.5s\n",
      "[CV] END ..............rf__max_depth=10, rf__n_estimators=50; total time=   0.2s\n",
      "[CV] END .............rf__max_depth=10, rf__n_estimators=100; total time=   0.4s\n",
      "[CV] END ..............rf__max_depth=5, rf__n_estimators=300; total time=   0.8s\n",
      "[CV] END .............rf__max_depth=10, rf__n_estimators=100; total time=   0.4s\n",
      "[CV] END ..............rf__max_depth=5, rf__n_estimators=300; total time=   0.8s\n",
      "[CV] END .............rf__max_depth=10, rf__n_estimators=100; total time=   0.4s\n",
      "[CV] END .............rf__max_depth=10, rf__n_estimators=100; total time=   0.4s\n",
      "[CV] END ..............rf__max_depth=5, rf__n_estimators=300; total time=   0.8s\n",
      "[CV] END .............rf__max_depth=10, rf__n_estimators=100; total time=   0.4s\n",
      "[CV] END .............rf__max_depth=10, rf__n_estimators=200; total time=   0.7s\n",
      "[CV] END .............rf__max_depth=10, rf__n_estimators=200; total time=   0.7s\n",
      "[CV] END ..............rf__max_depth=5, rf__n_estimators=300; total time=   0.8s\n",
      "[CV] END ..............rf__max_depth=15, rf__n_estimators=50; total time=   0.2s\n",
      "[CV] END ..............rf__max_depth=15, rf__n_estimators=50; total time=   0.2s\n",
      "[CV] END .............rf__max_depth=10, rf__n_estimators=200; total time=   0.7s\n",
      "[CV] END ..............rf__max_depth=15, rf__n_estimators=50; total time=   0.2s\n",
      "[CV] END ..............rf__max_depth=15, rf__n_estimators=50; total time=   0.2s\n",
      "[CV] END ..............rf__max_depth=15, rf__n_estimators=50; total time=   0.2s\n",
      "[CV] END .............rf__max_depth=10, rf__n_estimators=300; total time=   1.1s\n",
      "[CV] END .............rf__max_depth=10, rf__n_estimators=200; total time=   0.7s\n",
      "[CV] END .............rf__max_depth=15, rf__n_estimators=100; total time=   0.4s\n",
      "[CV] END .............rf__max_depth=10, rf__n_estimators=200; total time=   0.7s\n",
      "[CV] END .............rf__max_depth=15, rf__n_estimators=100; total time=   0.4s\n",
      "[CV] END .............rf__max_depth=10, rf__n_estimators=300; total time=   1.1s\n",
      "[CV] END .............rf__max_depth=15, rf__n_estimators=100; total time=   0.4s\n",
      "[CV] END .............rf__max_depth=15, rf__n_estimators=100; total time=   0.4s\n",
      "[CV] END .............rf__max_depth=15, rf__n_estimators=100; total time=   0.4s\n",
      "[CV] END .............rf__max_depth=10, rf__n_estimators=300; total time=   1.1s\n",
      "[CV] END .............rf__max_depth=15, rf__n_estimators=200; total time=   0.8s\n",
      "[CV] END .............rf__max_depth=15, rf__n_estimators=200; total time=   0.8s\n",
      "[CV] END .............rf__max_depth=15, rf__n_estimators=200; total time=   0.8s\n",
      "[CV] END .............rf__max_depth=10, rf__n_estimators=300; total time=   1.0s\n",
      "[CV] END .............rf__max_depth=10, rf__n_estimators=300; total time=   1.0s\n",
      "[CV] END .............rf__max_depth=15, rf__n_estimators=200; total time=   0.7s\n",
      "[CV] END .............rf__max_depth=15, rf__n_estimators=200; total time=   0.7s\n",
      "[CV] END .............rf__max_depth=15, rf__n_estimators=300; total time=   1.0s\n",
      "[CV] END .............rf__max_depth=15, rf__n_estimators=300; total time=   1.0s\n",
      "[CV] END .............rf__max_depth=15, rf__n_estimators=300; total time=   0.9s\n",
      "[CV] END .............rf__max_depth=15, rf__n_estimators=300; total time=   0.7s\n",
      "[CV] END .............rf__max_depth=15, rf__n_estimators=300; total time=   0.7s\n",
      "Best parameters for rf: {'rf__max_depth': 3, 'rf__n_estimators': 300}\n",
      "Best score for rf: -1.5952775985818701\n",
      "Running GridSearch for svm...\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "[CV] END .....................svm__C=0.1, svm__kernel=linear; total time=   0.0s\n",
      "[CV] END .....................svm__C=0.1, svm__kernel=linear; total time=   0.0s\n",
      "[CV] END .....................svm__C=0.1, svm__kernel=linear; total time=   0.0s\n",
      "[CV] END .....................svm__C=0.1, svm__kernel=linear; total time=   0.0s\n",
      "[CV] END .....................svm__C=0.1, svm__kernel=linear; total time=   0.0s\n",
      "[CV] END ........................svm__C=0.1, svm__kernel=rbf; total time=   0.0s\n",
      "[CV] END ........................svm__C=0.1, svm__kernel=rbf; total time=   0.0s\n",
      "[CV] END ........................svm__C=0.1, svm__kernel=rbf; total time=   0.0s\n",
      "[CV] END ........................svm__C=0.1, svm__kernel=rbf; total time=   0.0s\n",
      "[CV] END .......................svm__C=1, svm__kernel=linear; total time=   0.0s\n",
      "[CV] END ........................svm__C=0.1, svm__kernel=rbf; total time=   0.0s\n",
      "[CV] END .......................svm__C=1, svm__kernel=linear; total time=   0.0s\n",
      "[CV] END .......................svm__C=1, svm__kernel=linear; total time=   0.0s\n",
      "[CV] END .......................svm__C=1, svm__kernel=linear; total time=   0.0s\n",
      "[CV] END ..........................svm__C=1, svm__kernel=rbf; total time=   0.0s\n",
      "[CV] END ..........................svm__C=1, svm__kernel=rbf; total time=   0.0s\n",
      "[CV] END ..........................svm__C=1, svm__kernel=rbf; total time=   0.0s\n",
      "[CV] END .......................svm__C=1, svm__kernel=linear; total time=   0.0s\n",
      "[CV] END ..........................svm__C=1, svm__kernel=rbf; total time=   0.0s\n",
      "[CV] END .........................svm__C=10, svm__kernel=rbf; total time=   0.0s\n",
      "[CV] END ..........................svm__C=1, svm__kernel=rbf; total time=   0.0s\n",
      "[CV] END .........................svm__C=10, svm__kernel=rbf; total time=   0.0s\n",
      "[CV] END ......................svm__C=10, svm__kernel=linear; total time=   0.0s\n",
      "[CV] END ......................svm__C=10, svm__kernel=linear; total time=   0.1s\n",
      "[CV] END ......................svm__C=10, svm__kernel=linear; total time=   0.0s\n",
      "[CV] END .........................svm__C=10, svm__kernel=rbf; total time=   0.0s\n",
      "[CV] END .........................svm__C=10, svm__kernel=rbf; total time=   0.0s\n",
      "[CV] END .........................svm__C=10, svm__kernel=rbf; total time=   0.0s\n",
      "[CV] END ......................svm__C=10, svm__kernel=linear; total time=   0.0s\n",
      "[CV] END ......................svm__C=10, svm__kernel=linear; total time=   0.0s\n",
      "[CV] END ........................svm__C=100, svm__kernel=rbf; total time=   0.0s\n",
      "[CV] END ........................svm__C=100, svm__kernel=rbf; total time=   0.0s\n",
      "[CV] END ........................svm__C=100, svm__kernel=rbf; total time=   0.0s\n",
      "[CV] END ........................svm__C=100, svm__kernel=rbf; total time=   0.0s\n",
      "[CV] END ........................svm__C=100, svm__kernel=rbf; total time=   0.0s\n",
      "[CV] END .....................svm__C=100, svm__kernel=linear; total time=   0.2s\n",
      "[CV] END .....................svm__C=100, svm__kernel=linear; total time=   0.2s\n",
      "[CV] END .....................svm__C=100, svm__kernel=linear; total time=   0.3s\n",
      "[CV] END .....................svm__C=100, svm__kernel=linear; total time=   0.2s\n",
      "[CV] END .....................svm__C=100, svm__kernel=linear; total time=   0.5s\n",
      "Best parameters for svm: {'svm__C': 10, 'svm__kernel': 'rbf'}\n",
      "Best score for svm: -0.09242990686510741\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Sample columns for the example (you would replace these with actual column names from your dataset)\n",
    "categorical_columns = ['month', 'day']  # Replace with actual categorical columns\n",
    "numerical_columns = ['FFMC', 'DMC', 'DC', 'ISI', 'temp', 'RH', 'wind', 'rain']  # Replace with actual numerical columns\n",
    "\n",
    "# Define preprocessing for numerical and categorical features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_columns),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_columns)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define models for the pipelines (using regressors for a regression task)\n",
    "models = {\n",
    "    'linreg': LinearRegression(),\n",
    "    'dtree': DecisionTreeRegressor(),\n",
    "    'rf': RandomForestRegressor(),\n",
    "    'svm': SVR()\n",
    "}\n",
    "\n",
    "# Define hyperparameters to tune for each model\n",
    "param_grids = {\n",
    "    'linreg': {'linreg__fit_intercept': [True, False]},\n",
    "    'dtree': {'dtree__max_depth': [3, 5, 7, 10], 'dtree__min_samples_split': [2, 5, 10, 20]},\n",
    "    'rf': {'rf__n_estimators': [50, 100, 200, 300], 'rf__max_depth': [3, 5, 10, 15]},\n",
    "    'svm': {'svm__C': [0.1, 1, 10, 100], 'svm__kernel': ['linear', 'rbf']}\n",
    "}\n",
    "\n",
    "# Create a pipeline for each model and perform GridSearch\n",
    "for model_name, model in models.items():\n",
    "    # Build the pipeline for each model\n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor), (model_name, model)])\n",
    "    \n",
    "    # Define the GridSearchCV for each pipeline with the corresponding parameter grid\n",
    "    grid_search = GridSearchCV(pipeline, param_grids[model_name], cv=5, n_jobs=-1, verbose=2)\n",
    "    \n",
    "    print(f\"Running GridSearch for {model_name}...\")\n",
    "    \n",
    "    # Fit the GridSearchCV to your training data (replace X_train and y_train with your actual training data)\n",
    "    grid_search.fit(X_train, y_train)  # Replace X_train and y_train with your actual training data\n",
    "    \n",
    "    print(f\"Best parameters for {model_name}: {grid_search.best_params_}\")\n",
    "    print(f\"Best score for {model_name}: {grid_search.best_score_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate\n",
    "\n",
    "+ Which model has the best performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the linear regression model has the best performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export\n",
    "\n",
    "+ Save the best performing model to a pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into features (X) and target (y)\n",
    "X = data.drop(columns='area')  # Features (independent variables)\n",
    "y = data['area']              # Target (dependent variable)\n",
    "\n",
    "# Now, you can split the data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# You can now train a model using X_train, y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 11613.639591913126\n",
      "R-squared (R2): 0.014771466120020338\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# If you have columns with categorical data\n",
    "categorical_columns = ['month', 'day']  # replace with the actual categorical column names\n",
    "\n",
    "# Create a transformer for the categorical columns\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(), categorical_columns)\n",
    "    ], \n",
    "    remainder='passthrough'  # Keep other columns as they are\n",
    ")\n",
    "\n",
    "# Create a pipeline with preprocessing and model training\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', LinearRegression())\n",
    "])\n",
    "\n",
    "# Train the model using the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'Mean Squared Error (MSE): {mse}')\n",
    "print(f'R-squared (R2): {r2}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explain\n",
    "\n",
    "+ Use SHAP values to explain the following only for the best-performing model:\n",
    "\n",
    "    - Select an observation in your test set and explain which are the most important features that explain that observation's specific prediction.\n",
    "\n",
    "    - In general, across the complete training set, which features are the most and least important.\n",
    "\n",
    "+ If you were to remove features from the model, which ones would you remove? Why? How would you test that these features are actually enhancing model performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize the encoder\n",
    "le_month = LabelEncoder()\n",
    "le_day = LabelEncoder()\n",
    "\n",
    "# Apply LabelEncoder to both 'month' and 'day'\n",
    "X_train['month'] = le_month.fit_transform(X_train['month'])\n",
    "X_test['month'] = le_month.transform(X_test['month'])\n",
    "\n",
    "X_train['day'] = le_day.fit_transform(X_train['day'])\n",
    "X_test['day'] = le_day.transform(X_test['day'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(Answer here.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SHAP values provide a way to explain the output of machine learning models by attributing the prediction to the individual features. Each feature's SHAP value indicates how much that feature contributed to the difference between the predicted value and the mean predicted value. It helps us understand the model’s behavior and decision-making process for both individual predictions and across the entire dataset.\n",
    "Using SHAP values, we can determine which features contributed most to the model's prediction for that observation. Positive SHAP values indicate features that pushed the prediction higher, while negative SHAP values indicate features that pushed the prediction lower.\n",
    "SHAP values can be averaged across all observations in the dataset to give us a ranking of feature importance. This allows us to identify which features are most influential in making predictions across the entire training set.\n",
    "To understand which features are actually enhancing the model’s performance, we can remove features that have low importance (based on SHAP values). We would then retrain the model and compare performance metrics (e.g., RMSE, R-squared) before and after removing these features.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criteria\n",
    "\n",
    "The [rubric](./assignment_3_rubric_clean.xlsx) contains the criteria for assessment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Information\n",
    "\n",
    "🚨 **Please review our [Assignment Submission Guide](https://github.com/UofT-DSI/onboarding/blob/main/onboarding_documents/submissions.md)** 🚨 for detailed instructions on how to format, branch, and submit your work. Following these guidelines is crucial for your submissions to be evaluated correctly.\n",
    "\n",
    "### Submission Parameters:\n",
    "* Submission Due Date: `HH:MM AM/PM - DD/MM/YYYY`\n",
    "* The branch name for your repo should be: `assignment-3`\n",
    "* What to submit for this assignment:\n",
    "    * This Jupyter Notebook (assignment_3.ipynb) should be populated and should be the only change in your pull request.\n",
    "* What the pull request link should look like for this assignment: `https://github.com/<your_github_username>/production/pull/<pr_id>`\n",
    "    * Open a private window in your browser. Copy and paste the link to your pull request into the address bar. Make sure you can see your pull request properly. This helps the technical facilitator and learning support staff review your submission easily.\n",
    "\n",
    "Checklist:\n",
    "- [ ] Created a branch with the correct naming convention.\n",
    "- [ ] Ensured that the repository is public.\n",
    "- [ ] Reviewed the PR description guidelines and adhered to them.\n",
    "- [ ] Verify that the link is accessible in a private browser window.\n",
    "\n",
    "If you encounter any difficulties or have questions, please don't hesitate to reach out to our team via our Slack at `#cohort-3-help`. Our Technical Facilitators and Learning Support staff are here to help you navigate any challenges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n",
    "Cortez,Paulo and Morais,Anbal. (2008). Forest Fires. UCI Machine Learning Repository. https://doi.org/10.24432/C5D88D."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsi_participant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
